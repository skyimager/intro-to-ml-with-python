{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cd1ed0ab",
   "metadata": {},
   "source": [
    "# Outline for this section\n",
    "\n",
    "1. Deep learning - basics & reasoning\n",
    "    - learning problems\n",
    "    - representations\n",
    "    \n",
    "2. From biological to artificial neural networks\n",
    "    - neurons \n",
    "    - universal function approximation\n",
    "    \n",
    "3. components of ANNs\n",
    "    - building parts\n",
    "    - learning\n",
    "    \n",
    "4. ANN architectures\n",
    "    - Multilayer perceptrons\n",
    "    - Convolutional neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5617eaf0",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"./images/core_aspects_examples.png\" width=\"500\" height=\"280\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdc85c4",
   "metadata": {},
   "source": [
    "# General Understanding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b335f0",
   "metadata": {},
   "source": [
    "- as said before: `deep learning` is (a subset of) `machine learning` \n",
    "- it thus includes the core aspects we talked about in the [previous section]() and builds upon them:\n",
    "    - different learning problems and resulting models/architectures\n",
    "    - loss function & optimization\n",
    "    - training, evaluation, validation\n",
    "    - biases & problems\n",
    "    \n",
    "- this furthermore transfers to the key components you as a user has to think about\n",
    "    - objective function (What is the goal?)\n",
    "    - learning rule (How should weights be updated to improve the objective function?)\n",
    "    - network architecture (What are the network parts and how are they connected?)\n",
    "    - initialisation (How are weights initially defined?)\n",
    "    - environment (What kind of data is provided for/during the learning?)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2aedb11b",
   "metadata": {},
   "source": [
    "### Learning problems\n",
    "\n",
    "As in [machine learning]() in general, we have `supervised` & `unsupervised learning problems` again:\n",
    "\n",
    "<img align=\"center\" src=\"./images/supervised_unsupervised.png\" width=\"1200\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6afecd5e",
   "metadata": {},
   "source": [
    "### Complex Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443f2596",
   "metadata": {},
   "source": [
    "In learning data patterns, one major problem is the `variance` of the input we encounter which subsequently makes it very hard to find appropriate `transformations` that can lead to/help to achieve `generalizable behavior`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766f60fb",
   "metadata": {},
   "source": [
    "- let's assume we want to learn to recognize, label and predict \"cats\" based on a set of images that look like this\n",
    "\n",
    "<img align=\"center\" src=\"./images/cat_prototype.png\" alt=\"logo\" title=\"Github\" width=\"150\" height=\"250\" />\n",
    "\n",
    "- utilizing the `models` and `approaches` we talked about so far, we would use `predetermined transformations` (`features`) of our data `X`:\n",
    "\n",
    "<img align=\"center\" src=\"./images/cat_ml.png\" width=\"600\" height=\"280\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f7b8b4e",
   "metadata": {},
   "source": [
    "- however, this is by far not the only way we could encounter a cat ... there are a lots of sources of variation of our data `X`, including:\n",
    "\n",
    "- deformation\n",
    "\n",
    "<img align=\"center\" src=\"./images/cat_deformation.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "- occlusion\n",
    "\n",
    "<img align=\"center\" src=\"./images/cat_occlusion.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "- background clutter\n",
    "\n",
    "<img align=\"center\" src=\"./images/cat_background.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "- and intraclass variation\n",
    "\n",
    "<img align=\"center\" src=\"./images/cat_variation.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "- these variations (and many more) are usually not accounted for and our mapping from `X` to `Y` would fail"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02690e20",
   "metadata": {},
   "source": [
    "- what we want to learn to prevent this are `invariant representations` that capture `latent variables` which are variables you (most likely) cannot directly observe, but that affect the variables you can observe \n",
    "\n",
    "<img align=\"center\" src=\"./images/cat_dl.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "- the \"simple models\" we talked about so far work with `predetermined transformations` and thus perform `shallow learning`, more \"complex models\" perform `deep learning` in their `hidden layers` to learn `representations`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3424704",
   "metadata": {},
   "source": [
    "One important aspect to discuss here is another `inductive bias` we put into `models` (think about the `AI` set again) : the `hierarchical perception` of the `natural world`. In other words: the world around is `compositional` which means that the things we perceive are composed of smaller pieces, which themselves are composed of smaller pieces and so on ... .\n",
    "\n",
    "<img align=\"center\" src=\"./images/eickenberg_2016.png\" width=\"600\" height=\"400\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cf97e",
   "metadata": {},
   "source": [
    "The question is still: how do `ANN`s do that?\n",
    "\n",
    "- using biological neurons and networks as the basis for artificial neurons and networks might therefore also help to learn `invariant representations` that capture `latent variables`\n",
    "- `deep learning` = `representation learning`\n",
    "- our minds (most likely) contains `(invariant) representations` about the world that allow us to interact with it\n",
    "    - `task optimization`\n",
    "    - `generalizability` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee52cc9",
   "metadata": {},
   "source": [
    "## Perceptron Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ffc3dfe",
   "metadata": {},
   "source": [
    "<img src=\"images/logistic_classifier.png\"  height=\"600\" width=\"800\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fe7ce70",
   "metadata": {},
   "source": [
    "## Activation Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e234b",
   "metadata": {},
   "source": [
    "- the thing about `activation function`s...\n",
    "\n",
    "    - they define the resulting type of an `artificial neuron`\n",
    "    - thus they also define its capabilities\n",
    "    - require non-linearity\n",
    "        - because otherwise only linear functions and decision probabilities\n",
    "        \n",
    "$$\\begin{array}{l}\n",
    "\\text { Non-linear transfer functions}\\\\\n",
    "\\begin{array}{llc}\n",
    "\\hline \\text { Name } & \\text { Formula } & \\text { Year } \\\\\n",
    "\\hline \\text { none } & \\mathrm{y}=\\mathrm{x} & - \\\\\n",
    "\\text { sigmoid } & \\mathrm{y}=\\frac{1}{1+e^{-x}} & 1986 \\\\\n",
    "\\tanh & \\mathrm{y}=\\frac{e^{2 x}-1}{e^{2 x}+1} & 1986 \\\\\n",
    "\\text { ReLU } & \\mathrm{y}=\\max (\\mathrm{x}, 0) & 2010 \\\\\n",
    "\\text { (centered) SoftPlus } & \\mathrm{y}=\\ln \\left(e^{x}+1\\right)-\\ln 2 & 2011 \\\\\n",
    "\\text { LReLU } & \\mathrm{y}=\\max (\\mathrm{x}, \\alpha \\mathrm{x}), \\alpha \\approx 0.01 & 2011 \\\\\n",
    "\\text { maxout } & \\mathrm{y}=\\max \\left(W_{1} \\mathrm{x}+b_{1}, W_{2} \\mathrm{x}+b_{2}\\right) & 2013 \\\\\n",
    "\\text { APL } & \\mathrm{y}=\\max (\\mathrm{x}, 0)+\\sum_{s=1}^{S} a_{i}^{s} \\max \\left(0,-x+b_{i}^{s}\\right) & 2014 \\\\\n",
    "\\text { VLReLU } & \\mathrm{y}=\\max (\\mathrm{x}, \\alpha \\mathrm{x}), \\alpha \\in 0.1,0.5 & 2014 \\\\\n",
    "\\text { RReLU } & \\mathrm{y}=\\max (\\mathrm{x}, \\alpha \\mathrm{x}), \\alpha=\\operatorname{random}(0.1,0.5) & 2015 \\\\\n",
    "\\text { PReLU } & \\mathrm{y}=\\max (\\mathrm{x}, \\alpha \\mathrm{x}), \\alpha \\text { is learnable } & 2015 \\\\\n",
    "\\text { ELU } & \\mathrm{y}=\\mathrm{x}, \\text { if } \\mathrm{x} \\geq 0, \\text { else } \\alpha\\left(e^{x}-1\\right) & 2015 \\\\\n",
    "\\hline\n",
    "\\end{array}\n",
    "\\end{array}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "839655f7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-21T09:35:37.988565Z",
     "start_time": "2022-05-21T09:35:37.937151Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"700\"\n",
       "            height=\"400\"\n",
       "            src=\"https://polarisation.github.io/tfjs-activation-functions/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f8467188e90>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import IFrame\n",
    "IFrame(src='https://polarisation.github.io/tfjs-activation-functions/', width=700, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c2de2d2",
   "metadata": {},
   "source": [
    "- historically either [sigmoid](https://en.wikipedia.org/wiki/Logistic_function) or [tanh](https://en.wikipedia.org/wiki/Hyperbolic_function#Hyperbolic_tangent) utilized\n",
    "- even though they are [non-linear functions]() their properties make them insufficient for most problems, especially `sigmoid`\n",
    "    - rather simple `polynomials`  \n",
    "    - mainly work for `binary problems`\n",
    "    - computationally expensive\n",
    "    - they saturate causing the neuron and thus network to \"die\", i.e. stop `learning`\n",
    "- modern `ANN` frequently use `continuous activation functions` like [Rectified Linear Unit](https://deepai.org/machine-learning-glossary-and-terms/rectified-linear-units)\n",
    "    - doesn't saturate\n",
    "    - faster training and convergence\n",
    "    - introduce network sparsity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9a2bcc8",
   "metadata": {},
   "source": [
    "## Design"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6922512",
   "metadata": {},
   "source": [
    "Design a `neural network` so that for every possible input `X`, the outcome is `f(X)`.\n",
    "\n",
    "Here we introduce a [hidden layer]() that learns or more precisely `approximates` what those `transformations`/`functions` are on its own:\n",
    "\n",
    "Importantly, the [hidden layer]() consists of [artificial neurons]() that perceive `weighted inputs` `w` and perform [non-linear]() ([non-saturating]()) [activation functions]() `v` which `output` will be used for the `task` at hand\n",
    "\n",
    "<img align=\"center\" src=\"./images/UAT_hiddenlayer_function.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "It gets even better: this holds true even if there are multiple `inputs` and `outputs`:\n",
    "\n",
    "<img align=\"center\" src=\"./images/UAT_generalizability.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c92a9c",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"./images/ANN_layer.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "\n",
    "| Term         | Definition | \n",
    "|--------------|:-----:|\n",
    "| Layer |  Structure or network topology in the architecture of the model that consists of `nodes` and is connected to other layers, receiving and passing information. |\n",
    "| Input layer |  The layer that receives the external input data. |\n",
    "| Hidden layer(s) |  The layer(s) between `input` and `output layer` which performs `transformations` via `non-linear activation functions` . |\n",
    "| Output layer |  The layer that produces the final output/task. |\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636a6a44",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_subparts.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />\n",
    "\n",
    "\n",
    "| Term         | Definition | \n",
    "|--------------|:-----:|\n",
    "| Node |  `Artificial neurons`. |\n",
    "| Connection | Connection between `nodes`, providing `output` of one `node`/`neuron` as `input` to the next `node`/`neuron`.  |\n",
    "| Weight |  The relative importance of the `connection`. |\n",
    "| Bias |  The bias term that can be added to the `propagation function`, i.e. input to a neuron computed from the outputs of its predecessor neurons and their connections as a weighted sum. |\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3638b034",
   "metadata": {},
   "source": [
    "- `ANN`s can be described based on their amount of `hidden layers` (`depth`, `width`)\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_multilayer.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47f51a04",
   "metadata": {},
   "source": [
    "# Model Training/Fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "005fe75a",
   "metadata": {},
   "source": [
    "- when talking about `model fitting`, we need to talk about three central aspects:\n",
    "    - the `model`\n",
    "    - the `loss function`\n",
    "    - the `optimization`\n",
    "    \n",
    "| Term         | Definition | \n",
    "|--------------|:-----:|\n",
    "| Model |  A set of parameters that makes a prediction based on a given input. The parameter values are fitted to available data.|\n",
    "| Loss function | A function that evaluates how well your algorithm models your dataset |\n",
    "| Optimization | A function that tries to minimize the loss via updating model parameters. |\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fd65fa",
   "metadata": {},
   "source": [
    "#### An example: linear regression\n",
    "\n",
    "- Model:  $$y=\\beta_{0}+\\beta_{1} x_{1}^{2}+\\beta_{2} x_{2}^{2}$$\n",
    "- Loss function: $$ M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$\n",
    "- optimization: [Gradient descent]()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cbb772e",
   "metadata": {},
   "source": [
    "- `Gradient descent` with a `single input variable` and `n samples`\n",
    "    - Start with random weights (`β0` and `β1`) $$\\hat{y}_{i}=\\beta_{0}+\\beta_{1} X_{i}$$\n",
    "    - Compute loss (i.e. `MSE`) $$M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$\n",
    "    - Update `weights` based on the `gradient`\n",
    "    \n",
    "<img align=\"center\" src=\"https://cdn.hackernoon.com/hn-images/0*D7zG46WrdKx54pbU.gif\" alt=\"logo\" title=\"Github\" width=\"550\" height=\"280\" />\n",
    "<sub><sup><sub><sup><sup>https://cdn.hackernoon.com/hn-images/0*D7zG46WrdKx54pbU.gif\n",
    "</sup></sup></sub></sup></sub>\n",
    "\n",
    "- `Gradient descent` for complex models with `non-convex loss functions`\n",
    "    - Start with random weights (`β0` and `β1`) $$\\hat{y}_{i}=\\beta_{0}+\\beta_{1} X_{i}$$\n",
    "    - Compute loss (i.e. `MSE`) $$M S E=\\frac{1}{n} \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2}$$\n",
    "    - Update `weights` based on the `gradient`\n",
    "    \n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/gradient_descent_complex_models.png\" alt=\"logo\" title=\"Github\" width=\"500\" height=\"280\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9986f9a8",
   "metadata": {},
   "source": [
    "- to sufficiently talk about `learning` in `ANN`s we need to add a few things, however we heard some of them already \n",
    "    - `metric`\n",
    "    - `activation function`\n",
    "    - `weights`\n",
    "    - `batch size`\n",
    "    - `gradient descent`\n",
    "    - `backpropagation`\n",
    "    - `epoch`\n",
    "    - `regularization`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be94913",
   "metadata": {},
   "source": [
    "**Initialization of `weights` & `biases`**\n",
    "\n",
    "- Upon `building` our network we also need to `initialize` the `weights` and `biases`. \n",
    "- Both are important `hyper-parameters` for our `ANN` and the way it `learns` as they can help preventing `activation function outputs` from `exploding` or `vanishing` when moving through the `ANN`. \n",
    "- This relates directly to the `optimization` as the `loss gradient` might become too large or too small, prolonging the time the network needs to converge or even prevents it completely. Importantly, certain `initializers` work better with certain `activation functions`. For example: [tanh](https://en.wikipedia.org/wiki/Hyperbolic_functions#Hyperbolic_tangent) likes `Glorot/Xavier initialization` while [ReLu](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) likes `He initialization`. \n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_Cat_biases.png\" alt=\"logo\" title=\"Github\" width=\"600\" height=\"350\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654ff63f",
   "metadata": {},
   "source": [
    "**A journey through the `ANN`**\n",
    "\n",
    "The input is then processed by the `layers`, their `nodes` and respective `activation functions`, being passed through the `ANN`. Each `layer` and `node` will compute a certain `transformation` of the `input` it receives from the previous `layer` based on its `activation function` and `weights`/`biases`.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_connections.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67cf767",
   "metadata": {},
   "source": [
    "**The `output layer`**\n",
    "\n",
    "- After a while, we will reach the end of our `ANN`, the `output layer`. \n",
    "- As the last part of our `ANN`, it will produce the results we're interested in. Its number of `nodes` and `activation function` will depend on the `learning problem` at hand. \n",
    "- For a `binary classification task` it will have `2 nodes` corresponding to the both `classes` and might use `sigmoid` or [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function). \n",
    "- For `multiclass classification tasks` it will have as many `nodes` as there are `classes` and utilize the [softmax activation function](https://en.wikipedia.org/wiki/Softmax_function). \n",
    "- Both `sigmoid` and `softmax` are related to `logistic regression`, with the latter being a generalized form of it. Why does this matter? Our `output layer` will produce `real-valued scores` for each of the `classes` that are however not `scaled` and straightforward to interpret. \n",
    "- Using for example the `softmax function` we can transform these values into `scaled probability distributions` between `0` and `1` which values add up to `1` and can be submitted to other analysis pipelines or directly evaluated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4940f6d6",
   "metadata": {},
   "source": [
    "Lets assume our `ANN` is `trained` to recognize and distinguish `cats` and `capybaras`, meaning we have a `binary classification task`.  Defining `cats` as `class 1` and `capybaras` as `class 2` (not my opinion, just an example), the corresponding `vectors` we would like to obtain from the `output layer` would be `[1,0]` and `[0,1]` respectively. However, what we would get from the `output layer` in absence of e.g. `softmax`, would rather look like `[1.6, 0.2]` and `[0.4, 1.2]`. This is identical to what the penultimate `layer` would provide as `input` the `output` i.e. `softmax layer` if we had an additional layer just for that and not the respective `activation function`. \n",
    "\n",
    "After passing through the `softmax layer` or our `output layer` with `softmax activation function` the `real-valued scores` `[1.6, 0.2]` and `[0.4, 1.2]` would be (for example) `[0.802, 0.198]` and `[0.310, 0.699]`. Knowing it's now a `scaled probabilistic distribution` that can range between `0` and `1` and sums up to `1`, it's much easier to interpret.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_labels.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_softmax.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae2cfbf",
   "metadata": {},
   "source": [
    "**The `metric`**\n",
    "\n",
    "The index of the vector provided by the `softmax output layer` with the largest value will be treated as the `class` predicted by the `ANN`, which in our example would be \"cat\". The `ANN` will then use the `predicted class` and compare it to the `true class`, computing a `metric` to assess its performance. Remember folks: `deep learning` is `machine learning` and computing a `metric` is no exception to that. Thus, depending on your data and `learning problem` you can indicate a variety of `metrics` your `ANN` should utilize, including `accuracy`,  `F1`, `AUC`, etc. . Note: in `binary tasks` usually only the largest value is treated as a `class prediction`, this is called `Top-1 accuracy`. On the contrary, in `multiclass tasks` with many `classes` (animals, cell components, disease propagation types, etc.) quite often the largest `5` values are treated as `class predictions` and utilized within the `metric`, which is called `Top-5 accuracy`.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_accuracy.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917df914",
   "metadata": {},
   "source": [
    "**The `loss function`**\n",
    "\n",
    "Besides the `metric`, our `ANN` will also a compute a `loss function` that will quantify how far the `probabilities`, computed by the `softmax function` of the `output layer`, are away from the `true values` we want to achieve, i.e. the `classes`. As mentioned in the [introduction]() and comparable to the `metric`, the choice of `loss function` depends on the data you have and the `learning problem` you want to solve. If you want to `predict` `numerical values` you might want to employ a `regression` based approach and use `MSE` as the `loss function`. If you want to `predict` `classes` you might to employ a `classification` based approach and use a form of `cross-entropy` as the `loss function`.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_loss.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db640a22",
   "metadata": {},
   "source": [
    "**`Batch size`**\n",
    "\n",
    "As with other `machine learning` approaches, we will ideally have a `training`, `validation` and `test set`. One `hyperparameter` that is involved in this process and also can define our entire `learning process` is `batch size`. It defines the number of `samples` in the `training set` our `ANN` processes before `optimization` is used to update the `weights` based on the result of the `loss function`. For example, if our `training set` has `100 samples` and we set a `batch size` of `5`, we would divide the `training set` into `20 batches` of `5 samples` each. In turn this would mean that our `ANN` goes through `5 samples` before using `optimization` to update the `weights` and thus our `ANN` would update its `weights` `20` times during `training`.\n",
    "\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_batch.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "466bfb02",
   "metadata": {},
   "source": [
    "**The `optimization`**\n",
    "\n",
    "Once a `batch` has been processed by the `ANN` the `optimization algorithm` will get to work. As mentioned before, most `machine learning problems` utilize [gradient descent](https://en.wikipedia.org/wiki/Gradient_descent) as the `optimization algorithm`. As mentioned during the [introduction]() and a few slides above, we have an `objective function` we want to `optimize`, for example `minimizing` the `error` computed by our `cross-entropy loss function`.  So what happens is the following. At first, an entire `batch` is processed by the `ANN` and `accuracy` as well as `loss` are computed. \n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_gd.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8f796",
   "metadata": {},
   "source": [
    "Optimizers on surface:\n",
    "\n",
    "<div>\n",
    "<img src=\"images/gds_1.gif\"  height=\"600\" width=\"800\" />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5048c850",
   "metadata": {},
   "source": [
    "**`Backpropagation`**\n",
    "\n",
    "- Actually, `gradient descent` is part of something bigger called [backpropagation](https://en.wikipedia.org/wiki/Backpropagation). \n",
    "- Once we did a `forward pass` through the `ANN`, i.e. `data` goes from `input` to `output layer`, the `ANN` will use `backpropagation` to update the `model parameters`. \n",
    "- It does so by utilizing `gradient descent` and the [chain rule](https://en.wikipedia.org/wiki/Chain_rule) to `propagate` the `error` `backwards`. \n",
    "- Simply put: starting at the `output layer` `gradient descent` is applied to `update` its `parameters`, i.e. `weights` and `biases`, the `error` is re-computed through the `loss function` and `propagated backwards` to the previous `layer`, where `parameters` will be `updated`, the `error` re-computed through the `loss function` and so forth. \n",
    "- As `parameters` interact with each other, the application of the `chain rule` is important as it can decompose the `composition` of two `differentiable functions` into their `derivatives`.\n",
    "\n",
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_bp.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e570341",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_bp_2.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14469190",
   "metadata": {},
   "source": [
    "**The number of `epochs`**\n",
    "\n",
    "The duration of the `ANN` `training` is usually determined by the interplay between `batch sizes` and another `hyperparameter` called `epochs`. Whereas the `batch size` defines the number of `training set samples` to process before updating the `model parameters`, the number of `epochs` specifies how often the `ANN` should process the entire `training set`. Thus, once all `batches` have been processed, one `epoch` is over. The number of `epochs` is something you set when start the `training`, just like the `batch size`. Both are therefore `parameters` for the `training` and not `parameters` that are learned by the `training`. For example, if you have `100 samples`, a `batch size` of `10` and set the number of `epochs` to `500` your `ANN` will go through the entire `training set` `500` times, that is `5000 batches` and thus `5000 updates` to your `model`. While this sounds already like a lot, these numbers are more than small compared to that what \"real-life\" `ANN`s go through. There, these numbers are in the millions and beyond. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b11aee8",
   "metadata": {},
   "source": [
    "<img align=\"center\" src=\"https://raw.githubusercontent.com/PeerHerholz/ML-DL_workshop_SynAGE/master/lecture/static/ANN_cat_epoch.png\" alt=\"logo\" title=\"Github\" width=\"700\" height=\"450\" />\n",
    "\n",
    "\n",
    "Please note: this is of course only the theoretical duration in terms of `iterations` and not the actual duration it takes to `train` your `ANN`. This is quite often hard to `predict` (hehe, got it?) as it depends on the `computational setup` you're working with, the `data` and obviously the `model` and its `hyperparameters`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
