{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Neural networks are becoming increasing more popular and are responsible for some of the most cutting edge advancements in data science including image and speech recongintion. \n",
    "\n",
    "- They have also been transformative in reducing the need for time intesive feature engingeering, needed for traditional supervised learning tasks. \n",
    "\n",
    "- In this lecture, we'll further investigate the architecture of neural networks.\n",
    "\n",
    "### Objectives\n",
    "You will be able to:\n",
    "* Explain what neural networks are, and what they can achieve\n",
    "* Explain the basic architecture of a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a neural network?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start with an easy example to get an idea of what a neural network is:\n",
    "\n",
    "**Usecase:** Imagine a city has 10 ice cream vendors. We would like to predict what the sales amount is for an ice cream vendor given certain input features. \n",
    "Imagine you have several features to predict the sales for each ice cream vendor: \n",
    "- the location, \n",
    "- the way the ice cream is priced, and \n",
    "- the variety in the ice cream offerings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the input feature *location*: You know that one of the things that really affect the sales is how many people will walk by the ice cream shop, as these are all potential customers. And realistically, the volume of people passing is largely driven by the *location*. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's look at the input feature *pricing*. How the ice cream is priced really tells us something about the affordability, which will affect sales as well. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Last, let's look at the *variety in offering*. When an ice cream shop offers a lot of different ice cream flavors, this might be perceived as a higher quality shop just because customers have more flavors to choose from (and might really like that!). On the other hand, *pricing* might also affect perceived quality: customers might feel that the quality is higher when the prices are too. \n",
    "\n",
    "This shows that several inputs might affect one hidden feature (here quality is that feature), as these features in the so-called \"hidden layer\" are called. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In reality, all features will be connected with all nodes in the hidden layer, and weights will be assigned to the edges (more about this later), as you can see in the network below. That's why networks like this are also referred to as **densely connected neural networks**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Ice_cream_network_smaller.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we generalize all this, a neural network looks like the configuration below. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/First_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, to implement a neural network, we need to give feed it the inputs $x_i$ (location, pricing and variety in the example) and the outcome $y$ (pricing in the example), and all the features in the middle will be figured out automatically in the network. That's why this layer is called the **hidden layer**, with the nodes representing **hidden units**. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimensionality of the hidden layer is typically set as a user-defined hyperparameter. The more units we put into the hidden layer, the more complex functions the network is able to fit. But higher dimensionality comes at a cost for two reasons: \n",
    "1. more computation is required to learn the network parameters and make predictions; and \n",
    "2. a larger number of parameters is more prone to overfitting, resulting in a model with poor predictive performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The power of deep learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our previous example, we have 3 input units, hidden layer with 4 units and 1 output units. Notice that networks come in all shapes and sizes. This is only one example of what deep learning is capable of! The network described above can be extended almost endlessly:\n",
    "\n",
    "- We can add more features (nodes) in the input layer.\n",
    "- We can add more nodes in the hidden layer. Also, we can simply add more hidden layers. This is what turns a neural network in a \"deep\" neural network (hence, deep learning)\n",
    "- We can have several nodes in the output layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/Deeper_network.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And there is one more thing that makes deep learning extremely powerful: unlike many other statistical and machine learning techniques, deep learning can deal extremely well with **unstructured data**.\n",
    "\n",
    "\n",
    "In the ice cream vendor example, the input features can be seen as **structured data**. The input features very much take a form of a \"classical\" data set: observations are rows, features are columns. \n",
    "\n",
    "Examples or **unstructured data** however, are: images, audio files, text data, etc. Historically, and unlike humans, machines had a very hard time interpreting unstructured data. Deep learning was really able to drastically improve machine performance when using unstructured data!\n",
    "\n",
    "To illustrate the power of deep learning, we describe some applications of deep learning below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| x | y |\n",
    "|---|---|\n",
    "| features of an ice cream shop  | sales |\n",
    "| Pictures of cats vs dogs | cat or dog? |\n",
    "| Pictures of presidents | which president is it? |\n",
    "| Dutch text | English text |\n",
    "| audio files | text |\n",
    "|  ... | ... |         \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Types or Neural networks:\n",
    "- Standard neural networks\n",
    "- Convolutional neural networks (input = images, video)\n",
    "- Recurrent neural networks (input = audio files, text, time series data)\n",
    "- Generative adversarial networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An introductory example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Problem statement and matrix representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll see that there is quite a bit of theory and mathematical notation needed when using neural networks. We'll introduce all this for the first time by using an example.\n",
    "Imagine we have a data set with images. Some of them have Santa in it, others don't. We'll use a neural network to train the model so it can detect whether Santa is in a picture or not.\n",
    "\n",
    "As mentioned before, this is a kind of problem where the input data is composed of images. Now how does Python read images? To store an image, your computes stores 3 matrices which correspond with 3 color channels: red, green and blue (also referred to as RGB). The numbers in each of the three matrices correspond with the pixel intensity values in each of the three colors. The picture below denotes a hypothetical representation of a 4 x 4 pixel image (note that 4 x 4 is tiny, generally you'll have much bigger dimensions). Generally, pixel intensity values are on the scale [0,255]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/RGB_sm.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Having 3 matrices associated with one image, we'll need to modify this shape to get to one input feature vector. You'll want to \"unrow\" your input feature values into one so-called \"feature vector\". You should start with unrowing the red pixel matrix, then the green one, then the blue one. Unrowing the RGB matrices  in the image above would result in:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $x = \\begin{bmatrix} 35  \\\\ 19 \\\\  \\vdots \\\\ 9 \\\\7 \\\\\\vdots \\\\ 4 \\\\ 6 \\\\ \\vdots \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting feature vector is a matrix with 1 column and 4 x 4 x 3 = 48 rows. Let's introduce some more notation to formalize this all.\n",
    "\n",
    "$(x,y)$ = a training sample, where $x \\in  \\mathbb{R}^n , y \\in \\{0,1\\}$. Note that $n$ is the number of inputs in the feature vector (48 in the example).\n",
    "\n",
    "Let's say we have $l$ training samples. Your training set then looks like this: $\\{(x^{(1)},y^{(1)}), \\ldots, (x^{(l)},y^{(l)})\\}$\n",
    "Similarly, let's say the test set has $m$ test samples.\n",
    "\n",
    "Note that the resulting matrix $x$ has dimensions ($n$ x $l%$), and looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " $ \\hspace{1.1cm} x^{(1)} \\hspace{0.4cm} x^{(2)} \\hspace{1.4cm} x^{(l)} $\n",
    " \n",
    " $x $= $\\begin{bmatrix} 35 & 23 & \\cdots & 1\\\\ 19 & 88 &\\cdots & 230\\\\  \\vdots & \\vdots & \\ddots & \\vdots \\\\ 9 & 3 &\\cdots & 222 \\\\7 &166 &\\cdots  &43 \\\\ \\vdots & \\vdots & \\ddots & \\vdots  \\\\ 4 & 202 & \\cdots & 98 \\\\ 6 & 54 & \\cdots & 100 \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The training set labels matrix has dimensions $(1$ x $ l)$, and would look something like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$y $= $\\begin{bmatrix} 1 & 0 & \\cdots & 1 \\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where 1 means that the image contains a Santa, 0 means there is no Santa in the image.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression as a neural network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So how will we be able to predict wheather y is 0 or 1 for a certain image? You might remember from logistic regression models that the eventual predictor, $\\hat y$, is generally never exactly 0 or 1, but some value in between. \n",
    "\n",
    "Formally, you'll denote that $ \\hat y = P(y=1 \\mid x) $. Remember that $x \\in  \\mathbb{R}^n $. As in classical (logistic) regression we'll need some parameters. \n",
    "\n",
    "We'll need some expression here in order to make a prediction.\n",
    "The parameters here are $w \\in  \\mathbb{R}^n$ and $b \\in \\mathbb{R}$. Some expression to get to $\\hat y$ could be $\\hat y = w^T x + b$. The problem here is, however, that this type of expression does not ensure that the eventual outcome $ \\hat y$ will be between zero and one, and it could be much bigger than one or even negative!\n",
    "\n",
    "This is why a transformation of $w^T x + b$ is needed. For this particular example, we denote $\\hat y = \\sigma(w^T x + b)$, where $z = w^T x + b$, then $ \\hat y = \\sigma(z)$. This so-called *sigmoid function* is a popular *activation function* (more about activation functions later) in neural networks. With the expression for a sigmoid given by $\\sigma(z) = \\displaystyle\\frac{1}{1 + \\exp(-z)}$, it is clear that $\\sigma(z)$ will always be somewhere between 0 and 1, as you can see in the plot below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/sigmoid_smaller.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bringing all this together, the neural network can be represented as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/log_reg.png)\n",
    "**_“Input times weights , add Bias and Activate”_**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other popular activation functions include:\n",
    "- Relu\n",
    "- Softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-12-15T05:19:21.236991Z",
     "start_time": "2019-12-15T05:19:21.011586Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "def relu(z):\n",
    "    \"\"\"\n",
    "    Helper function to compute the element-wise ReLU activation\n",
    "    of an array by simple and efficient thresholding.\n",
    "    \"\"\"\n",
    "    \n",
    "    relu = z * (z > 0)\n",
    "    drelu = 1.0 * (z > 0)\n",
    "    \n",
    "    return relu, drelu\n",
    "\n",
    "# Plot example ReLU function\n",
    "z = np.linspace(-1.0, 1.0)\n",
    "fz, _ = relu(z)\n",
    "plt.plot(z, fz, lw=2)\n",
    "plt.xlabel('$z$', fontsize=10)\n",
    "plt.ylabel('$f(z)$', fontsize=10)\n",
    "plt.title('Rectified Linear Unit (ReLU) Function', fontsize=10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the loss and cost function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problem statement: given that we have $\\{(x^{(1)},y^{(1)}), \\ldots, (x^{(l)},y^{(l)})\\}$, we want to obtain $\\hat y \\approx y$. Neural networks use **loss** and **cost** functions here.\n",
    "\n",
    "The **loss function** is used to measure the inconsistency between the predicted value $(\\hat y)$ and the actual label $y$.\n",
    "\n",
    "In logistic regression the loss function is defined as: <br>\n",
    "\n",
    "$\\mathcal{L}(\\hat y, y) = - ( y \\log (\\hat y) + (1-y) \\log(1-\\hat y))$. \n",
    "\n",
    "The advantage of this loss function expression is that the optimization space here is convex, which makes optimizing using gradient descent easier. The loss function, however, is defined over 1 particular training sample. The cost function takes the average loss over all the samples: \n",
    "\n",
    "$J(w,b) = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1}\\mathcal{L}(\\hat y^{(i)}, y^{(i)})$\n",
    "\n",
    "When you train your logistic regression model, the purpose is to find parameters $w$ and $b$ such that your cost function is minimized!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
    "import numpy as np\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.gca(projection='3d')\n",
    "\n",
    "# Make data.\n",
    "X = np.arange(-5, 5, 0.1)\n",
    "Y = np.arange(-5, 5, 0.1)\n",
    "X, Y = np.meshgrid(X, Y)\n",
    "R = X**2+ Y**2 + 6\n",
    "\n",
    "# Plot the surface.\n",
    "surf = ax.plot_surface(X, Y, R, cmap=cm.coolwarm,\n",
    "                       linewidth=0, antialiased=False)\n",
    "\n",
    "# Customize the z axis.\n",
    "ax.set_zlim(0, 50)\n",
    "ax.zaxis.set_major_locator(LinearLocator(10))\n",
    "ax.zaxis.set_major_formatter(FormatStrFormatter('%.02f'))\n",
    "\n",
    "ax.set_xlabel('w', fontsize=12)\n",
    "ax.set_ylabel('b', fontsize=12)\n",
    "ax.set_zlabel('J(w,b)', fontsize=12)\n",
    "\n",
    "ax.set_yticklabels([])\n",
    "ax.set_xticklabels([])\n",
    "ax.set_zticklabels([])\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations! You have gotten to the point where you have the expression for the cost function and the loss function. The step we have just taken is called **forward propagation**.\n",
    "\n",
    "The cost function takes a convex form, looking much like this plot here! The idea is that you'll start with some initial values of $w$ and $b$, and then gradient descent, as you've seen before, takes a step in the steepest direction downhill.\n",
    "\n",
    "Looking at $w$ and $b$ separately, the idea of the algorithm is that both $w$ and $b$ will be updated repeatedly in each step:\n",
    "\n",
    "$w := w- \\alpha\\displaystyle \\frac{dJ(w)}{dw}$ and\n",
    "$b := b- \\alpha\\displaystyle \\frac{dJ(b)}{db}$\n",
    "\n",
    "Remember that $ \\displaystyle \\frac{dJ(w)}{dw}$ and $\\displaystyle \\frac{dJ(b)}{db}$ represent the *slope* of the function $J$ with respect to $w$ and $b$ respectively! We've never seen $\\alpha$ before, but for now you should just remember that this is denoted the *learning rate*. \n",
    "\n",
    "What I have just explained here is called **backpropagation**. You need to take the derivatives to calculate the difference between the desired and calculated outcome, and repeat these steps until you get to the lowest possible cost value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General Backpropogation Intuition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computational Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp1.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp2.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chain Rule"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp3.png\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computed Graph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp4.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Factorising Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp5.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp6.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp7.png\" width=\"400\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Forward-mode differentiation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp8.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse-mode differentiation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp9.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advantages of Reverse Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp10.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp11.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/bp12.png\" width=\"600\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mathematical Representation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the chain rule, computation graphs are popular. Imagine there are just 2 features $x_1$ and $x_2$. The graph going from our input variables to our loss function is given below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/log_reg_deriv.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You'll first want to compute the derivative to the loss with respect to $\\hat y$ first.\n",
    "\n",
    "This will be explained in more detail in class, but what you need to know is that you backpropagate, first:\n",
    "\n",
    "1) You'll want to go from $\\mathcal{L}(\\hat y , y)$ to $\\hat y = \\sigma (z)$. You can do this by taking the derivative of $\\mathcal{L}(\\hat y , y)$ with respect to $\\hat y$, and it can be shown that this is given by $\\displaystyle \\frac{d\\mathcal{L}(\\hat y , y)}{d \\hat y} = \\displaystyle \\frac{-y}{\\hat y}+\\displaystyle \\frac{1-y}{1-\\hat y}$\n",
    "\n",
    "2) As a next step you'll want to take the derivative with respect to z. It can be shown that $ dz = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d z} = \\hat y - y$. This derivative can also be written as $\\displaystyle\\frac{d\\mathcal{L}}{d\\hat y} \\displaystyle\\frac{d\\hat y}{dz} $.\n",
    "\n",
    "3) Last, and this is where you want to get to, you need to derive $\\mathcal{L}$ with respect to $w_1$, $w_2$ and $b$. It can be shown that: \n",
    "$dw_1 = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d w_1} = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d \\hat y}\\displaystyle\\frac{d\\hat y}{dz}\\displaystyle\\frac{dz}{d w_1} = x_1 dz $\n",
    "\n",
    "Similarly, it can be shown that:\n",
    "$dw_2 = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d w_2} = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d \\hat y}\\displaystyle\\frac{d\\hat y}{dz}\\displaystyle\\frac{dz}{d w_2} = x_2 dz $\n",
    "\n",
    "and\n",
    "\n",
    "$db = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d b} = \\displaystyle\\frac{d\\mathcal{L}(\\hat y , y)}{d \\hat y}\\displaystyle\\frac{d\\hat y}{dz}\\displaystyle\\frac{dz}{d b} = dz $\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "with $dw_1, dw_2$ and $db$ now known, you would go ahead and update \n",
    "\n",
    "$w_1 := w_1- \\alpha * d w_1$;\n",
    "\n",
    "$w_2 := w_2- \\alpha * d w_2$;\n",
    "\n",
    "$b := b - \\alpha * d b$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending to multiple samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this example just incorporates 1 training sample. Let'\n",
    "s look at how this is done when you have multiple ($l$) training samples!\n",
    "We basically want to compute the derivative of the overall cost function,\n",
    "\n",
    "$\\displaystyle \\frac{dJ(w,b)}{dw_i} = \\displaystyle\\frac{1}{l}\\displaystyle\\sum^l_{i=1} \\frac{d\\mathcal{L}(\\hat y^{(i)}, y^{(i)})}{dw_i}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at how we will get to the minimization of the cost function. As mentioned before, we'll have to initialize some values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize $J= 0$, $dw_1= 0$, $dw_2= 0$, $db= 0$. \n",
    "\n",
    "For each training sample $1,...,l$ you'll need to compute:\n",
    "\n",
    "$ z^{(i)} = w^T x^ {(i)} +b $\n",
    "\n",
    "$\\hat y^{(i)} = \\sigma (z^{(i)})$\n",
    "\n",
    "$dz^{(i)} = \\hat y^{(i)}- y^{(i)}$\n",
    "\n",
    "Then, you'll need to make update:\n",
    "\n",
    "$J_{+1} = - [y^{(i)} \\log (\\hat y^{(i)}) + (1-y^{(i)}) \\log(1-\\hat y^{(i)})$\n",
    "\n",
    "$dw_{1, +1}^{(i)} = x_1^{(i)} * dz^{(i)}$\n",
    "\n",
    "$dw_{2, +1}^{(i)} = x_2^{(i)} * dz^{(i)}$\n",
    "\n",
    "$db_{+1}^{(i)} =  dz^{(i)}$\n",
    "\n",
    "$\\dfrac{J}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{dw_1}{m}$, $\\dfrac{db}{m}$\n",
    "\n",
    "After that, update: \n",
    "\n",
    "$w_1 := w_1 - \\alpha dw_1$\n",
    "\n",
    "$w_2 := w_2 - \\alpha dw_2$\n",
    "\n",
    "$b := b - \\alpha db$\n",
    "\n",
    "repeat until convergence!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Conclusion: \n",
    "\n",
    "For each model iteration: \n",
    "- the model will pass the batch of images through the network, and predict their labels using the initialized (or previously set) weights and biases. \n",
    "- After predicting the value, it will calculate the residuals (i.e. the cost function value for the set of predicted y and actual y). \n",
    "- then it will apply gradient descent to estimate corrective gradients, then will update the values of weights and biases using these gradients."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- https://towardsdatascience.com/multi-layer-neural-networks-with-sigmoid-function-deep-learning-for-rookies-2-bf464f09eb7f\n",
    "\n",
    "- https://playground.tensorflow.org/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
